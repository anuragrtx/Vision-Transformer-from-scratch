# -*- coding: utf-8 -*-
"""ViT_from_scratch_on_CIFAR_10_PyTorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12xFviVX4o_q4aBoy4HHprLEbhaJQ0SZ3
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
import numpy as np
import random
import matplotlib.pyplot as plt

!nvidia-smi

# Setup Device-Agnostic Code

device = "cuda" if torch.cuda.is_available() else "cpu"
device

# Set the seed

torch.manual_seed(42)
torch.cuda.manual_seed(42)
random.seed(42)

# Settiing the hyperparameters
BATCH_SIZE = 128
EPOCHS = 70
LEARNING_RATE = 3e-4
IMAGE_SIZE = 32
CHANNELS = 3
EMBED_DIM = 256
NUM_HEADS = 8
PATCH_SIZE = 4
NUM_CLASSES = 10
DEPTH = 6
MLP_DIM = 512
DROP_RATE = 0.1
PATCH_SIZE = 4

#Define Image Transformations

transform = transforms.Compose([
                                transforms.ToTensor(),
                                transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
                                # Helps the model to converge faster and computations stable.
                                ])

# Datasets

train_dataset = datasets.CIFAR10(root = "data", train=True, download=True, transform=transform )

test_dataset = datasets.CIFAR10(root = "data", train=False, download=True, transform=transform )

train_dataset

test_dataset

# Datasets into Dataloaders
# DataLoader turns the data into batches or mini-batches.


train_loader = DataLoader(dataset=train_dataset,
                          batch_size=BATCH_SIZE,
                          shuffle=True)

test_loader = DataLoader(dataset=test_dataset,
                          batch_size=BATCH_SIZE,
                          shuffle=False)

# Checking

print(f"DataLoader: {train_loader, test_loader}")
print(f"Length of train_loader: {len(train_loader)} batches of {BATCH_SIZE}")
print(f"length of test_loader: {len(test_loader)} batches of {BATCH_SIZE}")

"""Building ViT from scratch"""

# ViT


class PatchEmbedding(nn.Module):
  def __init__(self,
               img_size,
               patch_size,
               in_channels,
               embed_dim):
    super().__init__()
    self.patch_size = patch_size
    self.proj = nn.Conv2d(in_channels=in_channels,
                          out_channels=embed_dim,
                          kernel_size=patch_size,
                          stride=patch_size)
    num_patches = (img_size // patch_size)**2
    self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
    self.pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, embed_dim))


  def forward(self, x:torch.Tensor):
    B = x.size(0)
    x = self.proj(x) #(B, E, H/P, W/P)
    x = x.flatten(2).transpose(1,2) #(B, N, E)
    cls_token = self.cls_token.expand(B, -1, -1)
    x = torch.cat((cls_token, x), dim=1)
    x = x + self.pos_embed
    return x

# Multilayer Percetions

class MLP(nn.Module):
  def __init__(self,
               in_features,
               hidden_features,
               drop_rate):
    super().__init__()
    self.fc1 = nn.Linear(in_features=in_features,
                         out_features=hidden_features)
    self.fc2 = nn.Linear(in_features=hidden_features,
                        out_features=in_features)
    self.dropout = nn.Dropout(drop_rate)

  def forward(self, x):
    x = self.dropout(F.gelu(self.fc1(x)))
    x = self.dropout(self.fc2(x))
    return x

# Transformer Encoder layer


class TransformerEncoderLayer(nn.Module):
  def __init__(self, embed_dim, num_heads, mlp_dim, drop_rate):
    super().__init__()
    self.norm1 = nn.LayerNorm(embed_dim)
    self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=drop_rate, batch_first=True)
    self.norm2 = nn.LayerNorm(embed_dim)
    self.mlp = MLP(embed_dim, mlp_dim, drop_rate)

  def forward(self, x):
    x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]
    x = x + self.mlp(self.norm2(x))
    return x

class VisionTransformer(nn.Module):
  def __init__(self, img_size, patch_size, in_channels, num_classes, embed_dim, depth, num_heads, mlp_dim, drop_rate):
    super().__init__()
    self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)
    self.encoder = nn.Sequential(
        *[TransformerEncoderLayer(embed_dim, num_heads, mlp_dim, drop_rate)
        for _ in range(depth)
    ])
    self.norm = nn.LayerNorm(embed_dim)
    self.head = nn.Linear(embed_dim, num_classes)


  def forward(self, x):
    x = self.patch_embed(x)
    x = self.encoder(x)
    x = self.norm(x)
    cls_token = x[:, 0]
    return self.head(cls_token)

# MODEL


model = VisionTransformer(
    IMAGE_SIZE, PATCH_SIZE, CHANNELS, NUM_CLASSES,
    EMBED_DIM, DEPTH, NUM_HEADS, MLP_DIM, DROP_RATE
).to(device)

model

"""Defining Loss funtion and an optimizer.

"""

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=model.parameters(),
                             lr=LEARNING_RATE)

criterion

optimizer

"""Training Loop Function"""

def train(model, loader, optimizer, criterion):
  # Set the mode of the model of the into training
  model.train()

  total_loss, correct = 0, 0

  for x, y in loader:
    x, y = x.to(device), y.to(device)     # Moving data to the target device
    optimizer.zero_grad()
    out = model(x)                 #  forward pass (mode outputs)
    loss = criterion(out, y)       #  calculate loss per batch
    loss.backward()                #  Backpropagation
    optimizer.step()               #  perform gradient descent

    total_loss += loss.item()*x.size(0)
    correct += (out.argmax(1) == y).sum().item()    # This counts how many predictions in the batch are correct. You keep adding that to correct to get total correct predictions across the epoch.

  return total_loss / len(loader.dataset), correct / len(loader.dataset)   # Normalizing lossacroll all batches

def evaluate(model, loader):
  model.eval()  # Set the mode of the model to evaluation
  correct = 0
  with torch.inference_mode():
    for x, y in loader:
      x, y = x.to(device), y.to(device)
      out = model(x)
      correct += (out.argmax(dim=1) == y).sum().item()
    return correct / len(loader.dataset)

from tqdm.auto import tqdm

# TRAINING

train_accuracies = []
test_accuracies = []

for epoch in tqdm(range(EPOCHS)):
  train_loss, train_acc = train(model, train_loader, optimizer, criterion)
  test_acc = evaluate(model, test_loader)
  train_accuracies.append(train_acc)
  test_accuracies.append(test_acc)
  print(f"Epoch: {epoch+1}/{EPOCHS}, Train loss: {train_loss: .4f}, Train acc: {train_acc: .4f}%, Test acc: {test_acc: .4f}")

train_accuracies

# Plot accuracy

plt.plot(train_accuracies, label="Train Accuracy")
plt.plot(test_accuracies, label="Test Accuracy")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.title("Training vs Test Accuracy")
plt.show()

print(f"Test dataset length: {len(test_dataset)}")

test_dataset[0][0].unsqueeze(dim=0).shape

import random
import torch
import matplotlib.pyplot as plt


print(">>> Before function call")
predict_and_plot_grid(model, test_dataset, classes=train_dataset.classes, grid_size=4)
print(">>> After function call")


# Unnormalize function

def unnormalize(img_tensor):
    mean = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(img_tensor.device)
    std = torch.tensor([0.5, 0.5, 0.5]).view(3, 1, 1).to(img_tensor.device)

    return img_tensor * std + mean



def predict_and_plot_grid(model, dataset, classes, grid_size=3):
    model.eval()

    fig, axes = plt.subplots(grid_size, grid_size, figsize=(9, 9))
    for i in range(grid_size):
        for j in range(grid_size):
            idx = random.randint(0, len(dataset)-1)
            img, true_label = dataset[idx]


            input_tensor = img.unsqueeze(0).to(device)


            with torch.inference_mode():
                output = model(input_tensor)
                _, predicted = torch.max(output.data, 1)



            img = unnormalize(img)

            img = img.clamp(0, 1)
            img = img.permute(1, 2, 0).cpu().numpy()

            truth = classes[true_label] == classes[predicted.item()]
            color = "g" if truth else "r"

            axes[i, j].imshow(img)
            axes[i, j].set_title(f"Truth: {classes[true_label]}\nPred: {classes[predicted.item()]}", fontsize=10, c=color)
            axes[i, j].axis("off")

    plt.tight_layout()
    plt.show()

img, label = test_dataset[0]
print(img.shape)

predict_and_plot_grid(model, test_dataset, classes=train_dataset.classes, grid_size=4)

import matplotlib.pyplot as plt

img, label = test_dataset[0]

# Normalized image (as it is stored)
plt.figure(figsize=(8, 4))
plt.subplot(1, 2, 1)
plt.imshow(img.permute(1, 2, 0).clamp(0, 1))  # just clamped for now
plt.title("As Stored (Normalized)")
plt.axis("off")

# Unnormalized
img_unnorm = img * 0.5 + 0.5  # since mean=0.5, std=0.5
plt.subplot(1, 2, 2)
plt.imshow(img_unnorm.permute(1, 2, 0).clamp(0, 1))
plt.title("Unnormalized")
plt.axis("off")
plt.show()

!pip install nbstripout
!nbstripout ViT_from_scratch_on_CIFAR_10_PyTorch.ipynb